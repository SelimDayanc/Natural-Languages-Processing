{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the file\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\",\"¿\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "  # remove extra space\n",
    "  w = w.strip()\n",
    "\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "<start> ¿ puedo tomar prestado este libro ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this @ book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n",
      "118964 118964\n"
     ]
    }
   ],
   "source": [
    "# Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "  return zip(*word_pairs)\n",
    "\n",
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])\n",
    "print(len(en), len(sp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize the sentence into list of words(integers) and pad the sequence to the same length\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "print(max_length_targ, max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n",
      "[   1   75  546   71 1594    3    2    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[  1  16  23 182  15 102   3   2   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
    "print(input_tensor_train[0])\n",
    "print(target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 16]), TensorShape([64, 11]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Configuration \n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "steps_per_epoch_val = len(input_tensor_val)//BATCH_SIZE\n",
    "embedding_dim = 256  # for word embedding\n",
    "units = 1024  # dimensionality of the output space of RNN\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(BUFFER_SIZE)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,  # Whether to return the last output in the output sequence, or the full sequence. \n",
    "                                   return_state=True,  # Whether to return the last state in addition to the output.\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "    return x, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape([[1,2,3],[4,5,6]], (-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 4935)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DotProductAttention(tf.keras.layers.Layer):\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # inner product, score shape == (batch_size, max_length, 1)\n",
    "    score = query_with_time_axis * values\n",
    "    score = tf.reduce_sum(score, axis=2)\n",
    "    score = tf.expand_dims(score, 2)\n",
    "    \n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attention_layer = DotProductAttention()\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(query_with_time_axis)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_layer = None):\n",
    "    super(DecoderWithAttention, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = attention_layer\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "    attention_weights = None\n",
    "    \n",
    "    if self.attention:\n",
    "      # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "      context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "      # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "      x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0)) \n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1.063386  1.3633859], shape=(2,), dtype=float32)\n",
      "tf.Tensor(1.2133859, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(loss_object([1,2],[[0,0.6,0.3,0.1],[0,0.6,0.3,0.1]]))\n",
    "print(loss_function([1,2],[[0,0.6,0.3,0.1],[0,0.6,0.3,0.1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def get_train_step_func():\n",
    "\n",
    "  @tf.function\n",
    "  def train_step(inp, targ, enc_hidden, encoder, decoder):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape: # for automatic differentiation\n",
    "      enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "      dec_hidden = enc_hidden\n",
    "\n",
    "      dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "      # Teacher forcing - feeding the target as the next input\n",
    "      for t in range(1, targ.shape[1]):\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "        loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "        # using teacher forcing\n",
    "        dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n",
    "    \n",
    "  return train_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def caculate_validation_loss(inp, targ, enc_hidden, encoder, decoder):\n",
    "  loss = 0\n",
    "  enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "  # Teacher forcing - feeding the target as the next input\n",
    "  for t in range(1, targ.shape[1]):\n",
    "    predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "    loss += loss_function(targ[:, t], predictions)\n",
    "    dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  loss = loss / int(targ.shape[1])\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_seq2seq(epochs, attention):\n",
    "  encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "  decoder = DecoderWithAttention(vocab_tar_size, embedding_dim, units, BATCH_SIZE, attention)\n",
    "  train_step_func = get_train_step_func()\n",
    "  training_loss = []\n",
    "  validation_loss = []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "      batch_loss = train_step_func(inp, targ, enc_hidden, encoder, decoder)\n",
    "      total_loss += batch_loss\n",
    "\n",
    "      if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss))\n",
    "        \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_val_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(validation_dataset.take(steps_per_epoch)):\n",
    "      val_loss = caculate_validation_loss(inp, targ, enc_hidden, encoder, decoder)\n",
    "      total_val_loss += val_loss\n",
    "\n",
    "    training_loss.append(total_loss / steps_per_epoch)\n",
    "    validation_loss.append(total_val_loss / steps_per_epoch_val)\n",
    "    print('Epoch {} Loss {:.4f} Validation Loss {:.4f}'.format(epoch + 1,\n",
    "                                        training_loss[-1], validation_loss[-1]))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "  return encoder, decoder, training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running seq2seq model without attention\n",
      "Epoch 1 Batch 0 Loss 4.6624\n",
      "Epoch 1 Batch 100 Loss 2.0467\n",
      "Epoch 1 Batch 200 Loss 1.9148\n",
      "Epoch 1 Batch 300 Loss 1.6339\n",
      "Epoch 1 Loss 1.9472 Validation Loss 1.5598\n",
      "Time taken for 1 epoch 2206.8120901584625 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "attention = None\n",
    "\n",
    "print(\"Running seq2seq model without attention\")\n",
    "encoder, decoder, training_loss, validation_loss = training_seq2seq(epochs, attention)\n",
    "\n",
    "tloss = training_loss\n",
    "vloss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running seq2seq model with dot product attention\n",
      "Epoch 1 Batch 0 Loss 4.6743\n",
      "Epoch 1 Batch 100 Loss 1.9057\n",
      "Epoch 1 Batch 200 Loss 1.6925\n",
      "Epoch 1 Batch 300 Loss 1.4807\n",
      "Epoch 1 Loss 1.8720 Validation Loss 1.4624\n",
      "Time taken for 1 epoch 2732.7333991527557 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=1\n",
    "attention = DotProductAttention()\n",
    "print(\"Running seq2seq model with dot product attention\")\n",
    "encoder_dp, decoder_dp, training_loss, validation_loss = training_seq2seq(epochs, attention)\n",
    "\n",
    "tloss = np.vstack((tloss, training_loss))\n",
    "vloss = np.vstack((vloss, validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running seq2seq model with Bahdanau attention\n",
      "Epoch 1 Batch 0 Loss 4.6387\n",
      "Epoch 1 Batch 100 Loss 1.7466\n",
      "Epoch 1 Batch 200 Loss 1.6278\n",
      "Epoch 1 Batch 300 Loss 1.2724\n",
      "Epoch 1 Loss 1.6922 Validation Loss 1.2578\n",
      "Time taken for 1 epoch 4048.851421356201 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 1\n",
    "\n",
    "attention = BahdanauAttention(units)\n",
    "print(\"Running seq2seq model with Bahdanau attention\")\n",
    "encoder_bah, decoder_bah, training_loss, validation_loss = training_seq2seq(epochs, attention)\n",
    "\n",
    "tloss = np.vstack((tloss, training_loss))\n",
    "vloss = np.vstack((vloss, validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation loss')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ax = plt.subplot(111) \n",
    "t = np.arange(1, epochs+1)\n",
    "\n",
    "for i in range(0, vloss.shape[0]):\n",
    "  line, = plt.plot(t, vloss[i,:], lw=2)\n",
    "\n",
    "ax.legend(('No attention', 'Dot product', 'Bahdanau'))\n",
    "ax.set_title(\"Validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    # until the predicted word is <end>.\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence\n",
    "\n",
    "    # the predicted ID is fed back into the model, no teacher forcing.\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> esta es mi vida . <end>\n",
      "Predicted translation: this is my life . <end> \n"
     ]
    }
   ],
   "source": [
    "result, sentence = translate(u'esta es mi vida.', encoder_bah, decoder_bah)\n",
    "print('Input: %s' % (sentence))\n",
    "print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> esta es mi vida . <end>\n",
      "Predicted translation: this is my book . <end> \n"
     ]
    }
   ],
   "source": [
    "result, sentence = translate(u'esta es mi vida.', encoder_dp, decoder_dp)\n",
    "print('Input: %s' % (sentence))\n",
    "print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ¿ todavia estan en casa ? <end>\n",
      "Predicted translation: are you getting home ? <end> \n"
     ]
    }
   ],
   "source": [
    "\n",
    "result, sentence = translate(u'¿todavia estan en casa?', encoder_bah, decoder_bah)\n",
    "print('Input: %s' % (sentence))\n",
    "print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
