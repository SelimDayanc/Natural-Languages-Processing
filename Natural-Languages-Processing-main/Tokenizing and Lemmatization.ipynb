{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#Tokenizing\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import MWETokenizer \n",
    "#Lemmatizationq\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Novel', 'by', 'Jack', 'London', ',', 'published', 'in', '1903', 'and', 'often', 'considered', 'to', 'be', 'his', 'masterpiece', '.', 'London', \"'s\", 'version', 'of', 'the', 'classic', 'quest', 'story', 'using', 'a', 'dog', 'as', 'the', 'protagobist', 'has', 'sometimes', 'been', 'erroneously', 'categorized', 'as', 'a', 'children', \"'s\", 'novel', '.']\n"
     ]
    }
   ],
   "source": [
    "#word_tokenize\n",
    "from nltk import word_tokenize\n",
    "text = \"Novel by Jack London, published in 1903 and often considered to be his masterpiece. London's version of the classic quest story using a dog as the protagobist has sometimes been erroneously categorized as a children's novel.\"\n",
    "nltk_tokens = nltk.word_tokenize(text)\n",
    "print (nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's\",\n",
       " 'better',\n",
       " 'to',\n",
       " 'be',\n",
       " 'fair',\n",
       " 'and',\n",
       " 'stay',\n",
       " 'alone',\n",
       " 'than',\n",
       " 'deviating',\n",
       " 'from',\n",
       " 'justice',\n",
       " 'to',\n",
       " 'ensure',\n",
       " 'that',\n",
       " 'most',\n",
       " 'people',\n",
       " 'go',\n",
       " 'with',\n",
       " 'you.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_tokenize\n",
    "text = \"\"\"It's better to be fair and stay alone than deviating from justice to ensure that most people go with you.\"\"\"\n",
    "text.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's better to be fair and stay alone than deviating from justice to ensure that most people go with you.\", 'Halide Edip Adıvar']\n"
     ]
    }
   ],
   "source": [
    "#Sent_tokenize\n",
    "from nltk import sent_tokenize\n",
    "text = \"It's better to be fair and stay alone than deviating from justice to ensure that most people go with you. Halide Edip Adıvar\"\n",
    "sentences=sent_tokenize(text)\n",
    "sentences\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's better to be fair and stay alone than deviating from justice to ensure that most people go with you\",\n",
       " ' Halide Edip Adıvar']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sent_tokenize\n",
    "text = \"\"\"It's better to be fair and stay alone than deviating from justice to ensure that most people go with you. Halide Edip Adıvar\"\"\"\n",
    "text.split('.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name_feyza_gizem_güler', 'and', 'School_number_2016555040']\n"
     ]
    }
   ],
   "source": [
    "#MWETokenizer \n",
    "from nltk.tokenize import MWETokenizer \n",
    "tk = MWETokenizer([('Name','feyza', 'gizem', 'güler'), ('School', 'number', '2016555040')]) \n",
    "text = \"Name feyza gizem güler and  School number 2016555040\"  \n",
    "result = tk.tokenize(text.split()) \n",
    "   \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jack_London', 'was', 'an', 'American_author', '.', 'He', 'was', 'a', 'pioneer', 'in', 'the', 'burgeoning', 'world_of', 'commercial_magazine_fiction', 'and', 'was', 'one_of_the', 'first_fiction_writers', 'to_obtain', 'worldwide_celebrity', 'and', 'a', 'large_fortune', 'from', 'his', 'fiction', 'alone', '.', 'Some_of', 'his', 'most_famous', 'works', 'include', 'The_Call_of_the_Wild']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import MWETokenizer   \n",
    "tk = MWETokenizer([('Jack','London'),('large','fortune'),( 'one','of','the'),('worldwide','celebrity'),('most','famous'),('The','Call','of','the','Wild'),('White','Fang'),('world','of'),('first','fiction','writers'),('to','obtain')]) \n",
    "tk.add_mwe(( 'commercial','magazine','fiction'))\n",
    "tk.add_mwe(( 'Some','of'))\n",
    "tk.add_mwe(('American','author'))\n",
    "text = \"Jack London was an American author . He was a pioneer in the burgeoning world of commercial magazine fiction and was one of the first fiction writers to obtain worldwide celebrity and a large fortune from his fiction alone . Some of his most famous works include The Call of the Wild\"\n",
    "result = tk.tokenize(text.split()) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using -> using\n",
      "feet -> foot\n",
      "shoulders -> shoulder\n",
      "springiness -> springiness\n",
      "thighs -> thigh\n",
      "memories -> memory\n",
      "sometimes -> sometimes\n",
      "legs -> leg\n",
      "miles -> mile\n",
      "flying -> flying\n"
     ]
    }
   ],
   "source": [
    "#wnl.lemmatize()\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wnl = WordNetLemmatizer()  \n",
    "list = ['using', 'feet', 'shoulders', 'springiness', 'thighs',  \n",
    "         'memories', 'sometimes', 'legs', 'miles','flying'] \n",
    "for words in list: \n",
    "    print(words + \" -> \" + wnl.lemmatize(words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['starting', 'from', 'a', 'dispute', 'as', 'to', 'which', 'should', 'chop', 'a', 'few', 'sticks', 'for', 'the', 'fire', ',', 'presently', 'would', 'be', 'lugged', 'in', 'the', 'rest', 'of', 'the', 'family', ',', 'fathers', ',', 'mothers', ',', 'uncles', ',', 'cousins', ',', 'people', 'thousands', 'of', 'miles', 'away', ',', 'and', 'some', 'of', 'them', 'dead', '.']\n",
      "starting from a dispute a to which should chop a few stick for the fire , presently would be lugged in the rest of the family , father , mother , uncle , cousin , people thousand of mile away , and some of them dead .\n"
     ]
    }
   ],
   "source": [
    "text = 'starting from a dispute as to which should chop a few sticks for the fire , presently would be lugged in the rest of the family, fathers,mothers,uncles,cousins,people thousands of miles away, and some of them dead.'\n",
    "word_list = nltk.word_tokenize(text) \n",
    "print(word_list) \n",
    "lemmatized_text = ' '.join([wnl.lemmatize(words) for words in word_list]) \n",
    "print(lemmatized_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "using\n",
      "be\n",
      "using\n",
      "be\n",
      "using\n"
     ]
    }
   ],
   "source": [
    "#pos tagging \n",
    "from nltk import WordNetLemmatizer\n",
    "wnl=WordNetLemmatizer()\n",
    "list=[\"am\",\"is\",\"are\"]\n",
    "for words in list:\n",
    "    print(wnl.lemmatize(words,pos=\"v\"))\n",
    "    print(wnl.lemmatize('using',pos='n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('playing', 'VBG'), ('play', 'NN'), ('played', 'VBN'), ('on', 'IN'), ('cats', 'NNS'), ('the', 'DT')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "wnl=WordNetLemmatizer()\n",
    "tag=nltk.pos_tag([\"playing\",\"play\",\"played\",\"on\",\"cats\",\"the\"])\n",
    "print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our', 'great', 'ideal', 'is', 'to', 'raise', 'our', 'nation', 'to', 'the', 'highest', 'standards', 'of', 'civilization', 'and', 'prosperity', ',', 'Mustafa', 'Kemal', 'Atatürk']\n",
      "[('Our', 'PRP$'), ('great', 'JJ'), ('ideal', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('raise', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('to', 'TO'), ('the', 'DT'), ('highest', 'JJS'), ('standards', 'NNS'), ('of', 'IN'), ('civilization', 'NN'), ('and', 'CC'), ('prosperity', 'NN'), (',', ','), ('Mustafa', 'NNP'), ('Kemal', 'NNP'), ('Atatürk', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "wnl = WordNetLemmatizer() \n",
    "def pos_tag(nltk_tag): \n",
    "    if nltk_tag.startswith('V'): \n",
    "        return wordnet.VERB \n",
    "    elif nltk_tag.startswith('N'): \n",
    "        return wordnet.NOUN \n",
    "    elif nltk_tag.startswith('R'): \n",
    "        return wordnet.ADV \n",
    "    elif nltk_tag.startswith('J'): \n",
    "        return wordnet.ADJ \n",
    "    else:           \n",
    "        return None\n",
    "text = 'Our great ideal is to raise our nation to the highest standards of civilization and prosperity , Mustafa Kemal Atatürk'\n",
    "t_words=nltk.word_tokenize(text)\n",
    "print(t_words)\n",
    "pos_tag = nltk.pos_tag(nltk.word_tokenize(text))   \n",
    "print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'very', 'beautiful', 'young', 'lady', 'is', 'walking', 'on', 'the', 'beach']\n",
      "[('A', 'DT'), ('very', 'RB'), ('beautiful', 'JJ'), ('young', 'JJ'), ('lady', 'NN'), ('is', 'VBZ'), ('walking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('beach', 'NN')]\n",
      "[('A', 'DT'), ('very', 'RB'), ('beautiful', 'JJ'), ('young', 'JJ'), ('lady', 'NN'), ('is', 'VBZ'), ('walking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('beach', 'NN')]\n",
      "[('A', 'DT'), ('very', 'RB'), ('beautiful', 'JJ'), ('young', 'JJ'), ('lady', 'NN'), ('is', 'VBZ'), ('walking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('beach', 'NN')]\n",
      "[('A', 'DT'), ('very', 'RB'), ('beautiful', 'JJ'), ('young', 'JJ'), ('lady', 'NN'), ('is', 'VBZ'), ('walking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('beach', 'NN')]\n",
      "[('A', 'DT'), ('very', 'RB'), ('beautiful', 'JJ'), ('young', 'JJ'), ('lady', 'NN'), ('is', 'VBZ'), ('walking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('beach', 'NN')]\n",
      "[('A', 'DT'), ('very', 'RB'), ('beautiful', 'JJ'), ('young', 'JJ'), ('lady', 'NN'), ('is', 'VBZ'), ('walking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('beach', 'NN')]\n",
      "[('A', 'DT'), ('very', 'RB'), ('beautiful', 'JJ'), ('young', 'JJ'), ('lady', 'NN'), ('is', 'VBZ'), ('walking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('beach', 'NN')]\n",
      "[('A', 'DT'), ('very', 'RB'), ('beautiful', 'JJ'), ('young', 'JJ'), ('lady', 'NN'), ('is', 'VBZ'), ('walking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('beach', 'NN')]\n",
      "[('A', 'DT'), ('very', 'RB'), ('beautiful', 'JJ'), ('young', 'JJ'), ('lady', 'NN'), ('is', 'VBZ'), ('walking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('beach', 'NN')]\n",
      "[('A', 'DT'), ('very', 'RB'), ('beautiful', 'JJ'), ('young', 'JJ'), ('lady', 'NN'), ('is', 'VBZ'), ('walking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('beach', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#pos tagging \n",
    "text=\"A very beautiful young lady is walking on the beach\"\n",
    "t_words=nltk.word_tokenize(text)\n",
    "print(t_words)\n",
    "for words in t_words:\n",
    "    tag=nltk.pos_tag(t_words)\n",
    "    print(tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
